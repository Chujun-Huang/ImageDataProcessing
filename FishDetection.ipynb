{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Install dependencies and\n",
    "!pip install albumentations==0.4.6\n",
    "!pip install pycocotools --quiet\n",
    "\n",
    "# Clone TorchVision repo and copy helper files\n",
    "!git clone https://github.com/pytorch/vision.git\n",
    "%cd vision\n",
    "!git checkout v0.3.0\n",
    "%cd ..\n",
    "!cp vision/references/detection/utils.py ./\n",
    "!cp vision/references/detection/transforms.py ./\n",
    "!cp vision/references/detection/coco_eval.py ./\n",
    "!cp vision/references/detection/engine.py ./\n",
    "!cp vision/references/detection/coco_utils.py ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# basic python and ML Libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# We will be reading images using OpenCV\n",
    "import cv2\n",
    "\n",
    "# matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# torchvision libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms as torchtrans\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# helper libraries\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "# for image augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# defining the files directory and testing directory\n",
    "files_dir = '/content/drive/MyDrive/20230604_WH4/TraingData/test.train'\n",
    "test_dir = '/content/drive/MyDrive/20230604_WH4/TestData'\n",
    "\n",
    "# we create a Dataset class which has a __getitem__ function and a __len__ function\n",
    "class ConeImagesDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, files_dir, width, height, transforms=None):\n",
    "    self.transforms = transforms\n",
    "    self.files_dir = files_dir\n",
    "    self.height = height\n",
    "    self.width = width\n",
    "\n",
    "    # sorting the images for consistency\n",
    "    # To get images, the extension of the filename is checked to be jpg\n",
    "    self.imgs = [image for image in sorted(os.listdir(files_dir)) if image[-4:]=='.png']\n",
    "\n",
    "    # classes: 0 index is reserved for background\n",
    "    self.classes = [_, 'cone']\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    img_name = self.imgs[idx]\n",
    "    image_path = os.path.join(self.files_dir, img_name)\n",
    "\n",
    "    # reading the images and converting them to correct size and color\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "    img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n",
    "    # diving by 255\n",
    "    img_res /= 255.0\n",
    "\n",
    "    # annotation file\n",
    "    annot_filename = img_name[:-4] + '.txt'\n",
    "    annot_file_path = os.path.join(self.files_dir, annot_filename)\n",
    "\n",
    "    boxes = []\n",
    "    labels = []\n",
    "\n",
    "    # cv2 image gives size as height x width\n",
    "    wt = img.shape[1]\n",
    "    ht = img.shape[0]\n",
    "\n",
    "    # box coordinates for xml files are extracted and corrected for image size given\n",
    "    with open(annot_file_path) as f:\n",
    "      for line in f:\n",
    "        labels.append(1)\n",
    "\n",
    "        parsed = [float(x) for x in line.split(' ')]\n",
    "        x_center = parsed[1]\n",
    "        y_center = parsed[2]\n",
    "        box_wt = parsed[3]\n",
    "        box_ht = parsed[4]\n",
    "\n",
    "        xmin = x_center - box_wt/2\n",
    "        xmax = x_center + box_wt/2\n",
    "        ymin = y_center - box_ht/2\n",
    "        ymax = y_center + box_ht/2\n",
    "\n",
    "        xmin_corr = int(xmin*self.width)\n",
    "        xmax_corr = int(xmax*self.width)\n",
    "        ymin_corr = int(ymin*self.height)\n",
    "        ymax_corr = int(ymax*self.height)\n",
    "\n",
    "        boxes.append([xmin_corr, ymin_corr, xmax_corr, ymax_corr])\n",
    "\n",
    "    # convert boxes into a torch.Tensor\n",
    "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "    # getting the areas of the boxes\n",
    "    area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "    # suppose all instances are not crowd\n",
    "    iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "    labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "    target = {}\n",
    "    target[\"boxes\"] = boxes\n",
    "    target[\"labels\"] = labels\n",
    "    target[\"area\"] = area\n",
    "    target[\"iscrowd\"] = iscrowd\n",
    "    image_id = torch.tensor([idx])\n",
    "    target[\"image_id\"] = image_id\n",
    "\n",
    "    if self.transforms:\n",
    "      sample = self.transforms(image = img_res,\n",
    "                                bboxes = target['boxes'],\n",
    "                                labels = labels)\n",
    "      img_res = sample['image']\n",
    "      target['boxes'] = torch.Tensor(sample['bboxes'])\n",
    "\n",
    "    return img_res, target\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.imgs)\n",
    "\n",
    "\n",
    "# check dataset\n",
    "dataset = ConeImagesDataset(files_dir, 224, 224)\n",
    "print('Length of dataset:', len(dataset), '\\n')\n",
    "\n",
    "# getting the image and target for a test index.  Feel free to change the index.\n",
    "img, target = dataset[14]\n",
    "print('Image shape:', img.shape)\n",
    "print('Label example:', target)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to visualize bounding boxes in the image\n",
    "def plot_img_bbox(img, target):\n",
    "  # plot the image and bboxes\n",
    "  # Bounding boxes are defined as follows: x-min y-min width height\n",
    "  fig, a = plt.subplots(1,1)\n",
    "  fig.set_size_inches(5,5)\n",
    "  a.imshow(img)\n",
    "  for box in (target['boxes']):\n",
    "    x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n",
    "    rect = patches.Rectangle(\n",
    "      (x, y),\n",
    "      width, height,\n",
    "      linewidth = 1,\n",
    "      edgecolor = 'r',\n",
    "      facecolor = 'none'\n",
    "    )\n",
    "    # Draw the bounding box on top of the image\n",
    "    a.add_patch(rect)\n",
    "  #plt.savefig('/content/drive/MyDrive/20230604_WH4/TestData/Testlabel/Testindex14.png')\n",
    "  plt.show()\n",
    "\n",
    "# plotting the image with bboxes. Feel free to change the index\n",
    "img, target = dataset[14]\n",
    "plot_img_bbox(img, target)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Send train=True for training transforms and False for val/test transforms\n",
    "def get_transform(train):\n",
    "  if train:\n",
    "    return A.Compose(\n",
    "      [\n",
    "        A.HorizontalFlip(0.5),\n",
    "        # ToTensorV2 converts image to pytorch tensor without div by 255\n",
    "        ToTensorV2(p=1.0)\n",
    "      ],\n",
    "      bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']}\n",
    "    )\n",
    "  else:\n",
    "    return A.Compose(\n",
    "      [ToTensorV2(p=1.0)],\n",
    "      bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']}\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# use our dataset and defined transformations\n",
    "dataset = ConeImagesDataset(files_dir, 480, 480, transforms=get_transform(train=True))\n",
    "dataset_test = ConeImagesDataset(files_dir, 480, 480, transforms=get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "\n",
    "# train test split\n",
    "test_split = 0.2\n",
    "tsize = int(len(dataset)*test_split)\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-tsize])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-tsize:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "  dataset,\n",
    "  batch_size=10,\n",
    "  shuffle=True,\n",
    "  num_workers=0,\n",
    "  collate_fn=utils.collate_fn,\n",
    ")\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "  dataset_test,\n",
    "  batch_size=10,\n",
    "  shuffle=False,\n",
    "  num_workers=0,\n",
    "  collate_fn=utils.collate_fn,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_object_detection_model(num_classes):\n",
    "  # load a model pre-trained pre-trained on COCO\n",
    "  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "  iou_threshold = 0.7\n",
    "  model.roi_heads.nms_thresh = iou_threshold\n",
    "  # get number of input features for the classifier\n",
    "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "  # replace the pre-trained head with a new one\n",
    "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "  return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train on gpu if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "num_classes = 2 # one class (class 0) is dedicated to the \"background\"\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_object_detection_model(num_classes)\n",
    "#print(model)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "  optimizer,\n",
    "  step_size=3,\n",
    "  gamma=0.1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# training for 5 epochs\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # training for one epoch\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# the function takes the original prediction and the iou threshold.\n",
    "def apply_nms(orig_prediction, iou_thresh=0.3):\n",
    "  # torchvision returns the indices of the bboxes to keep\n",
    "  keep = torchvision.ops.nms(orig_prediction['boxes'].cpu(), orig_prediction['scores'].cpu(), iou_thresh)\n",
    "\n",
    "  final_prediction = orig_prediction\n",
    "  final_prediction['boxes'] = final_prediction['boxes'].cpu()[keep]\n",
    "  final_prediction['scores'] = final_prediction['scores'].cpu()[keep]\n",
    "  final_prediction['labels'] = final_prediction['labels'].cpu()[keep]\n",
    "  array = final_prediction['boxes'].numpy()\n",
    "  np.savetxt('/content/drive/MyDrive/20230604_WH4/TraingData/predictbbox/DSC_8682_adj-2.txt', array)\n",
    "\n",
    "  return final_prediction\n",
    "\n",
    "# function to convert a torchtensor back to PIL image\n",
    "def torch_to_pil(img):\n",
    "  return torchtrans.ToPILImage()(img).convert('RGB')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_img_bbox(img, target):\n",
    "  # plot the image and bboxes\n",
    "  # Bounding boxes are defined as follows: x-min y-min width height\n",
    "  fig, a = plt.subplots(1,1)\n",
    "  fig.set_size_inches(5,5)\n",
    "  a.imshow(img)\n",
    "  for box in (target['boxes']):\n",
    "    x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n",
    "    rect = patches.Rectangle(\n",
    "      (x, y),\n",
    "      width, height,\n",
    "      linewidth = 1,\n",
    "      edgecolor = 'r',\n",
    "      facecolor = 'none'\n",
    "    )\n",
    "    # Draw the bounding box on top of the image\n",
    "    a.add_patch(rect)\n",
    "  #plt.savefig('/content/drive/MyDrive/20230604_WH4/TestData/Test.pridect/Testpredict14.png')\n",
    "  plt.show()\n",
    "\n",
    "test_dataset = ConeImagesDataset(test_dir, 480, 480, transforms= get_transform(train=True))\n",
    "\n",
    "# pick one image from the test set\n",
    "img, target = test_dataset[0]\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  prediction = model([img.to(device)])[0]\n",
    "\n",
    "\n",
    "print('MODEL OUTPUT\\n')\n",
    "nms_prediction = apply_nms(prediction, iou_thresh=0.3)\n",
    "\n",
    "plot_img_bbox(torch_to_pil(img), nms_prediction)\n",
    "\n",
    "print(img.shape)\n",
    "print(img.size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_dataset = ConeImagesDataset(test_dir, 480, 480, transforms= get_transform(train=True))\n",
    "\n",
    "\n",
    "model.eval()\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "  test_dataset,\n",
    "  batch_size=10,\n",
    "  shuffle=False,\n",
    "  num_workers=4,\n",
    "  collate_fn=utils.collate_fn,\n",
    ")\n",
    "\n",
    "evaluate(model, data_loader_test, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}